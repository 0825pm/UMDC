#!/usr/bin/env python3
"""
Collect MVREC per-category results from CsvLogger output.
Parses the CSV files generated by MVREC's run.py and produces:
  1. Clean JSON file with per-category + average results
  2. Pretty-printed comparison table in terminal
  3. Summary CSV for easy import

Usage:
    python collect_mvrec_results.py --exp_name percategory_eval --output_dir ./results_mvrec
    
    # Or manually specify CSV path:
    python collect_mvrec_results.py --csv_path OUTPUT/MVREC/percategory_eval/results.csv
"""

import os
import csv
import json
import ast
import argparse
from datetime import datetime
from collections import defaultdict


# Category name mapping: MVREC dataset name → clean category name
DATASET_TO_CATEGORY = {
    "mvtec_bottle_data": "bottle",
    "mvtec_cable_data": "cable",
    "mvtec_capsule_data": "capsule",
    "mvtec_carpet_data": "carpet",
    "mvtec_grid_data": "grid",
    "mvtec_hazelnut_data": "hazelnut",
    "mvtec_leather_data": "leather",
    "mvtec_leather": "leather",
    "mvtec_metal_nut_data": "metal_nut",
    "mvtec_pill_data": "pill",
    "mvtec_pill": "pill",
    "mvtec_screw_data": "screw",
    "mvtec_tile_data": "tile",
    "mvtec_transistor_data": "transistor",
    "mvtec_wood_data": "wood",
    "mvtec_zipper_data": "zipper",
}

CATEGORY_ORDER = [
    "bottle", "cable", "capsule", "carpet", "grid", "hazelnut", "leather",
    "metal_nut", "pill", "screw", "tile", "transistor", "wood", "zipper"
]


def find_csv_path(exp_name):
    """Find the results CSV from MVREC's output structure."""
    # MVREC saves to OUTPUT/{PROJECT_NAME}/{exp_name}/results.csv
    # PROJECT_NAME is typically derived from the code directory
    possible_roots = ["OUTPUT", "output", "."]
    
    for root in possible_roots:
        if not os.path.isdir(root):
            continue
        for dirpath, dirnames, filenames in os.walk(root):
            if "results.csv" in filenames and exp_name in dirpath:
                return os.path.join(dirpath, "results.csv")
    
    return None


def parse_results_csv(csv_path):
    """Parse MVREC's results.csv into structured data.
    
    CSV columns typically include:
    - data_name: dataset name (e.g., mvtec_bottle_data)
    - mv_acc: multi-view accuracy dict {'mean': x, 'std': y, ...}
    - womv_acc: without-multi-view accuracy dict
    - run_name: experiment identifier
    - param_name: parameter set name
    - hyper_param: hyperparameter dict
    """
    results = defaultdict(lambda: defaultdict(dict))
    
    with open(csv_path, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            data_name = row.get('data_name', '')
            run_name = row.get('run_name', '')
            
            # Parse mv_acc (main metric)
            mv_acc_str = row.get('mv_acc', '{}')
            try:
                mv_acc = ast.literal_eval(mv_acc_str)
                if isinstance(mv_acc, dict):
                    mean = mv_acc.get('mean', 0)
                    std = mv_acc.get('std', 0)
                else:
                    mean = float(mv_acc)
                    std = 0
            except (ValueError, SyntaxError):
                try:
                    mean = float(mv_acc_str)
                    std = 0
                except ValueError:
                    continue
            
            # Map dataset name to clean category
            category = DATASET_TO_CATEGORY.get(data_name, data_name)
            
            # Extract k_shot and classifier from run_name
            # Format: MVRec-mso-EchoClassfierF-ks5-acti1
            k_shot = None
            classifier = None
            if 'ks' in run_name:
                parts = run_name.split('-')
                for p in parts:
                    if p.startswith('ks'):
                        try:
                            k_shot = int(p[2:])
                        except ValueError:
                            pass
                for p in parts:
                    if 'Echo' in p:
                        classifier = p
            
            if k_shot is None:
                continue
            
            method_key = f"{classifier}_ks{k_shot}"
            results[method_key][category] = {
                'mean': round(mean * 100, 2),  # Convert to percentage
                'std': round(std * 100, 2),
                'run_name': run_name,
                'k_shot': k_shot,
                'classifier': classifier,
            }
    
    return dict(results)


def parse_average_csv(csv_path):
    """Parse average_results.csv for summary data."""
    avg_path = csv_path.replace('results.csv', 'average_results.csv')
    if not os.path.exists(avg_path):
        return {}
    
    results = {}
    with open(avg_path, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            run_name = row.get('run_name', '')
            # Collect per-category values
            cat_results = {}
            for dataset_name, cat_name in DATASET_TO_CATEGORY.items():
                if cat_name in row or dataset_name in row:
                    key = cat_name if cat_name in row else dataset_name
                    try:
                        val = float(row[key])
                        cat_results[cat_name] = round(val * 100, 2)
                    except (ValueError, KeyError):
                        pass
            
            if 'average' in row:
                try:
                    cat_results['AVERAGE'] = round(float(row['average']) * 100, 2)
                except ValueError:
                    pass
            
            results[run_name] = cat_results
    
    return results


def compute_averages(results):
    """Compute average accuracy for each method."""
    for method, cats in results.items():
        valid_means = [v['mean'] for k, v in cats.items() if k in CATEGORY_ORDER]
        if valid_means:
            cats['AVERAGE'] = {
                'mean': round(sum(valid_means) / len(valid_means), 2),
                'std': 0,
                'k_shot': next(iter(cats.values())).get('k_shot'),
                'classifier': next(iter(cats.values())).get('classifier'),
            }
    return results


def print_comparison_table(results):
    """Print a clean comparison table."""
    if not results:
        print("No results found!")
        return
    
    methods = sorted(results.keys())
    
    # Header
    cat_width = 14
    col_width = 18
    
    print("\n" + "=" * (cat_width + col_width * len(methods) + 4))
    print("MVREC PER-CATEGORY RESULTS")
    print("=" * (cat_width + col_width * len(methods) + 4))
    
    # Method headers
    header = f"{'Category':<{cat_width}}"
    for m in methods:
        header += f"  {m:>{col_width-2}}"
    print(header)
    print("─" * (cat_width + col_width * len(methods) + 4))
    
    # Per-category rows
    for cat in CATEGORY_ORDER:
        row = f"{cat:<{cat_width}}"
        for m in methods:
            if cat in results[m]:
                mean = results[m][cat]['mean']
                std = results[m][cat]['std']
                val = f"{mean:.2f}% ±{std:.2f}"
            else:
                val = "  —"
            row += f"  {val:>{col_width-2}}"
        print(row)
    
    # Average row
    print("─" * (cat_width + col_width * len(methods) + 4))
    row = f"{'AVERAGE':<{cat_width}}"
    for m in methods:
        if 'AVERAGE' in results[m]:
            mean = results[m]['AVERAGE']['mean']
            val = f"{mean:.2f}%"
        else:
            val = "  —"
        row += f"  {val:>{col_width-2}}"
    print(row)
    print("=" * (cat_width + col_width * len(methods) + 4))


def save_results(results, output_dir):
    """Save results to JSON and CSV."""
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # JSON
    json_path = os.path.join(output_dir, f"mvrec_results_{timestamp}.json")
    with open(json_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nJSON saved: {json_path}")
    
    # Clean summary CSV
    csv_path = os.path.join(output_dir, f"mvrec_summary_{timestamp}.csv")
    methods = sorted(results.keys())
    with open(csv_path, 'w', newline='') as f:
        writer = csv.writer(f)
        header = ['Category'] + methods
        writer.writerow(header)
        
        for cat in CATEGORY_ORDER + ['AVERAGE']:
            row = [cat]
            for m in methods:
                if cat in results[m]:
                    row.append(f"{results[m][cat]['mean']:.2f}")
                else:
                    row.append('')
            writer.writerow(row)
    
    print(f"CSV saved:  {csv_path}")
    
    # Also save a "latest" symlink
    latest_json = os.path.join(output_dir, "mvrec_results_latest.json")
    latest_csv = os.path.join(output_dir, "mvrec_summary_latest.csv")
    
    # Copy to latest
    import shutil
    shutil.copy2(json_path, latest_json)
    shutil.copy2(csv_path, latest_csv)
    print(f"Latest:     {latest_json}")


def main():
    parser = argparse.ArgumentParser(description='Collect MVREC per-category results')
    parser.add_argument('--exp_name', type=str, default='percategory_eval',
                        help='Experiment name to search for')
    parser.add_argument('--csv_path', type=str, default=None,
                        help='Direct path to results.csv (overrides exp_name search)')
    parser.add_argument('--output_dir', type=str, default='./results_mvrec',
                        help='Output directory for collected results')
    args = parser.parse_args()
    
    # Find CSV
    if args.csv_path:
        csv_path = args.csv_path
    else:
        csv_path = find_csv_path(args.exp_name)
    
    if csv_path is None or not os.path.exists(csv_path):
        print(f"ERROR: Could not find results CSV for exp_name='{args.exp_name}'")
        print("Try specifying --csv_path directly, e.g.:")
        print("  python collect_mvrec_results.py --csv_path OUTPUT/MVREC/best_exp_train/results.csv")
        
        # Try to find any CSV files
        print("\nSearching for CSV files in OUTPUT/...")
        for dirpath, _, filenames in os.walk("OUTPUT"):
            for f in filenames:
                if f.endswith('.csv'):
                    print(f"  Found: {os.path.join(dirpath, f)}")
        return
    
    print(f"Parsing: {csv_path}")
    
    # Parse
    results = parse_results_csv(csv_path)
    
    if not results:
        # Try average_results.csv format
        avg_results = parse_average_csv(csv_path)
        if avg_results:
            print("Using average_results.csv format")
            # Convert to our format
            for run_name, cats in avg_results.items():
                method_results = {}
                for cat, val in cats.items():
                    if cat != 'AVERAGE':
                        method_results[cat] = {'mean': val, 'std': 0, 'run_name': run_name}
                if method_results:
                    results[run_name] = method_results
    
    if not results:
        print("ERROR: No results parsed from CSV!")
        print("CSV contents preview:")
        with open(csv_path, 'r') as f:
            for i, line in enumerate(f):
                if i < 5:
                    print(f"  {line.strip()}")
        return
    
    # Compute averages
    results = compute_averages(results)
    
    # Display
    print_comparison_table(results)
    
    # Save
    save_results(results, args.output_dir)


if __name__ == '__main__':
    main()
